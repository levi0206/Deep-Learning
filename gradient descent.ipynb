{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Implementation\n",
    "We will display the details of the implementation of gradient descents with PyTorch. For the mechanism of `torch.autograd`, please refer to `autograd.ipynb`.\n",
    "\n",
    "Consider a simple regression model in which \n",
    "- `x`: input\n",
    "- `y`: target\n",
    "- `w`: weight \n",
    "- `b`: bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-dimension model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(0.5)\n",
    "y = torch.tensor(3.0) # target, or label\n",
    "\n",
    "# Optimize the weight and bias -> requires_grad=True\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.3,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "def model(x,w,b):\n",
    "    return w * x + b\n",
    "y_pred = model(x,w,b)\n",
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "same_seed(2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred):\n",
    "    return (y_pred-y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8400, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "L = loss(y_pred)\n",
    "print(L)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .backward()\n",
    "Denote the loss function by $L$. Now we use `.backward()` to calculate the gradient $\\frac{\\partial}{\\partial w}L$ and $\\frac{\\partial}{\\partial b}L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient with respect to w: tensor(-2.2000)\n",
      "gradient with respect to b: tensor(-4.4000)\n"
     ]
    }
   ],
   "source": [
    "L.backward()\n",
    "print('gradient with respect to w:',w.grad)\n",
    "print('gradient with respect to b:',b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to record the loss\n",
    "loss_history = []\n",
    "loss_history.append(L)\n",
    "# hyperparameter\n",
    "accuracy = 0.5\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you have to use `torch.no_grad()` to disable the gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w after one update: tensor(1.0220, requires_grad=True)\n",
      "b after one update: tensor(0.3440, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad\n",
    "    print('w after one update:',w)\n",
    "    print('b after one update:',b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .grad.zero_()\n",
    "Before your next update, use `w.grad.zero_()` and `b.grad.zero_()` to clear the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients of w after .zero_(): tensor(0.)\n",
      "gradients of b after .zero_(): tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_() \n",
    "b.grad.zero_()\n",
    "print('gradients of w after .zero_():',w.grad)\n",
    "print('gradients of b after .zero_():',b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters after one update\n",
      "w: tensor(1.0220, requires_grad=True)\n",
      "b: tensor(0.3440, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Parameters after one update')\n",
    "print('w:',w)\n",
    "print('b:',b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "Together with for loop, we can optimize our model up to any acuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters after one update\n",
      "w: tensor(1.0434, requires_grad=True)\n",
      "b: tensor(0.3869, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.0644, requires_grad=True)\n",
      "b: tensor(0.4287, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.0848, requires_grad=True)\n",
      "b: tensor(0.4695, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.1046, requires_grad=True)\n",
      "b: tensor(0.5093, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.1240, requires_grad=True)\n",
      "b: tensor(0.5480, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.1429, requires_grad=True)\n",
      "b: tensor(0.5858, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.1613, requires_grad=True)\n",
      "b: tensor(0.6227, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.1793, requires_grad=True)\n",
      "b: tensor(0.6586, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.1968, requires_grad=True)\n",
      "b: tensor(0.6937, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.2139, requires_grad=True)\n",
      "b: tensor(0.7278, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.2306, requires_grad=True)\n",
      "b: tensor(0.7611, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.2468, requires_grad=True)\n",
      "b: tensor(0.7936, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.2626, requires_grad=True)\n",
      "b: tensor(0.8253, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.2781, requires_grad=True)\n",
      "b: tensor(0.8561, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.2931, requires_grad=True)\n",
      "b: tensor(0.8862, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.3078, requires_grad=True)\n",
      "b: tensor(0.9156, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.3221, requires_grad=True)\n",
      "b: tensor(0.9442, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.3360, requires_grad=True)\n",
      "b: tensor(0.9721, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.3496, requires_grad=True)\n",
      "b: tensor(0.9993, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.3629, requires_grad=True)\n",
      "b: tensor(1.0258, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.3758, requires_grad=True)\n",
      "b: tensor(1.0516, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.3884, requires_grad=True)\n",
      "b: tensor(1.0769, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4007, requires_grad=True)\n",
      "b: tensor(1.1014, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4127, requires_grad=True)\n",
      "b: tensor(1.1254, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4244, requires_grad=True)\n",
      "b: tensor(1.1488, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4358, requires_grad=True)\n",
      "b: tensor(1.1715, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4469, requires_grad=True)\n",
      "b: tensor(1.1938, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4577, requires_grad=True)\n",
      "b: tensor(1.2154, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4683, requires_grad=True)\n",
      "b: tensor(1.2365, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4786, requires_grad=True)\n",
      "b: tensor(1.2571, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4886, requires_grad=True)\n",
      "b: tensor(1.2772, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.4984, requires_grad=True)\n",
      "b: tensor(1.2968, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5079, requires_grad=True)\n",
      "b: tensor(1.3158, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5172, requires_grad=True)\n",
      "b: tensor(1.3344, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5263, requires_grad=True)\n",
      "b: tensor(1.3526, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5351, requires_grad=True)\n",
      "b: tensor(1.3703, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5438, requires_grad=True)\n",
      "b: tensor(1.3875, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5522, requires_grad=True)\n",
      "b: tensor(1.4043, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5604, requires_grad=True)\n",
      "b: tensor(1.4207, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5683, requires_grad=True)\n",
      "b: tensor(1.4367, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5761, requires_grad=True)\n",
      "b: tensor(1.4523, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5837, requires_grad=True)\n",
      "b: tensor(1.4675, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5911, requires_grad=True)\n",
      "b: tensor(1.4823, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.5984, requires_grad=True)\n",
      "b: tensor(1.4967, requires_grad=True)\n",
      "Parameters after one update\n",
      "w: tensor(1.6054, requires_grad=True)\n",
      "b: tensor(1.5108, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "while L >= accuracy:\n",
    "    y_pred = model(x,w,b)\n",
    "    L = loss(y_pred)\n",
    "    loss_history.append(L)\n",
    "    \n",
    "    L.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    w.grad.zero_() \n",
    "    b.grad.zero_()\n",
    "    print('Parameters after one update')\n",
    "    print('w:',w)\n",
    "    print('b:',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4957, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_history[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-dimension model \n",
    "Now we consider vector-valued input `X`, label `Y` and bias `B` with weight matrix `W`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor((0.2,0.5,0.1,3))\n",
    "Y = torch.tensor((1,2,3,4))\n",
    "W = torch.rand((4,4),requires_grad=True)\n",
    "B = torch.rand(4,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.5000, 0.1000, 3.0000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4290, 0.7201, 0.9481, 0.4797],\n",
       "        [0.5414, 0.9906, 0.4086, 0.2183],\n",
       "        [0.1834, 0.2852, 0.7813, 0.1048],\n",
       "        [0.6550, 0.8375, 0.1823, 0.5239]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2432, 0.9644, 0.5034, 0.0320], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2228, 2.2638, 1.0752, 2.1717], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(X,W,B):\n",
    "    return torch.matmul(W,X)+B\n",
    "Y_pred = model(X,W,B)\n",
    "Y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and float tensor\n",
    "We use `MSELoss` to be our loss. Note that the input of `nn.MSELoss()` should be float tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1531, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "Loss_history = []\n",
    "\n",
    "Y_pred = Y_pred.to(torch.float32)\n",
    "Y = Y.to(torch.float32)\n",
    "\n",
    "L = loss(Y_pred,Y)\n",
    "Loss_history.append(L)\n",
    "\n",
    "print(L)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient with respect to W:\n",
      "tensor([[ 0.1223,  0.3057,  0.0611,  1.8342],\n",
      "        [ 0.0264,  0.0660,  0.0132,  0.3957],\n",
      "        [-0.1925, -0.4812, -0.0962, -2.8872],\n",
      "        [-0.1828, -0.4571, -0.0914, -2.7425]])\n",
      "gradient with respect to B:\n",
      "tensor([ 0.6114,  0.1319, -0.9624, -0.9142])\n"
     ]
    }
   ],
   "source": [
    "L.backward()\n",
    "print('gradient with respect to W:')\n",
    "print(W.grad)\n",
    "print('gradient with respect to B:')\n",
    "print(B.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients of W after .zero_():\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "gradients of B after .zero_():\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "W.grad.zero_() \n",
    "B.grad.zero_()\n",
    "print('gradients of W after .zero_():')\n",
    "print(W.grad)\n",
    "print('gradients of B after .zero_():')\n",
    "print(B.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4278, 0.7170, 0.9475, 0.4613],\n",
      "        [0.5411, 0.9899, 0.4085, 0.2144],\n",
      "        [0.1853, 0.2900, 0.7822, 0.1337],\n",
      "        [0.6568, 0.8421, 0.1832, 0.5513]], requires_grad=True)\n",
      "B\n",
      "tensor([0.2370, 0.9631, 0.5130, 0.0412], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4266, 0.7141, 0.9470, 0.4439],\n",
      "        [0.5409, 0.9893, 0.4083, 0.2106],\n",
      "        [0.1871, 0.2946, 0.7831, 0.1611],\n",
      "        [0.6585, 0.8464, 0.1841, 0.5773]], requires_grad=True)\n",
      "B\n",
      "tensor([0.2312, 0.9619, 0.5222, 0.0499], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4255, 0.7114, 0.9464, 0.4274],\n",
      "        [0.5406, 0.9887, 0.4082, 0.2070],\n",
      "        [0.1888, 0.2989, 0.7840, 0.1870],\n",
      "        [0.6602, 0.8505, 0.1849, 0.6020]], requires_grad=True)\n",
      "B\n",
      "tensor([0.2257, 0.9607, 0.5308, 0.0581], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4245, 0.7087, 0.9459, 0.4118],\n",
      "        [0.5404, 0.9881, 0.4081, 0.2037],\n",
      "        [0.1905, 0.3030, 0.7848, 0.2117],\n",
      "        [0.6617, 0.8544, 0.1857, 0.6254]], requires_grad=True)\n",
      "B\n",
      "tensor([0.2205, 0.9596, 0.5390, 0.0659], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4235, 0.7063, 0.9454, 0.3969],\n",
      "        [0.5402, 0.9876, 0.4080, 0.2005],\n",
      "        [0.1920, 0.3069, 0.7856, 0.2350],\n",
      "        [0.6632, 0.8581, 0.1864, 0.6476]], requires_grad=True)\n",
      "B\n",
      "tensor([0.2156, 0.9585, 0.5468, 0.0733], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4226, 0.7039, 0.9449, 0.3828],\n",
      "        [0.5400, 0.9871, 0.4079, 0.1974],\n",
      "        [0.1935, 0.3106, 0.7864, 0.2572],\n",
      "        [0.6646, 0.8616, 0.1872, 0.6687]], requires_grad=True)\n",
      "B\n",
      "tensor([0.2109, 0.9575, 0.5542, 0.0803], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4217, 0.7017, 0.9445, 0.3695],\n",
      "        [0.5398, 0.9866, 0.4078, 0.1945],\n",
      "        [0.1949, 0.3141, 0.7871, 0.2782],\n",
      "        [0.6659, 0.8650, 0.1878, 0.6886]], requires_grad=True)\n",
      "B\n",
      "tensor([0.2064, 0.9565, 0.5612, 0.0870], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4208, 0.6996, 0.9441, 0.3568],\n",
      "        [0.5396, 0.9862, 0.4077, 0.1918],\n",
      "        [0.1962, 0.3174, 0.7877, 0.2982],\n",
      "        [0.6672, 0.8681, 0.1884, 0.7076]], requires_grad=True)\n",
      "B\n",
      "tensor([0.2022, 0.9556, 0.5679, 0.0933], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4200, 0.6976, 0.9437, 0.3448],\n",
      "        [0.5394, 0.9857, 0.4076, 0.1892],\n",
      "        [0.1975, 0.3206, 0.7883, 0.3171],\n",
      "        [0.6684, 0.8711, 0.1890, 0.7255]], requires_grad=True)\n",
      "B\n",
      "tensor([0.1982, 0.9547, 0.5742, 0.0993], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4193, 0.6957, 0.9433, 0.3334],\n",
      "        [0.5393, 0.9853, 0.4075, 0.1868],\n",
      "        [0.1987, 0.3236, 0.7889, 0.3350],\n",
      "        [0.6695, 0.8740, 0.1896, 0.7426]], requires_grad=True)\n",
      "B\n",
      "tensor([0.1944, 0.9539, 0.5802, 0.1049], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4185, 0.6939, 0.9429, 0.3226],\n",
      "        [0.5391, 0.9849, 0.4075, 0.1844],\n",
      "        [0.1998, 0.3264, 0.7895, 0.3520],\n",
      "        [0.6706, 0.8767, 0.1902, 0.7587]], requires_grad=True)\n",
      "B\n",
      "tensor([0.1908, 0.9531, 0.5858, 0.1103], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4179, 0.6922, 0.9426, 0.3123],\n",
      "        [0.5390, 0.9846, 0.4074, 0.1822],\n",
      "        [0.2009, 0.3291, 0.7901, 0.3682],\n",
      "        [0.6716, 0.8792, 0.1907, 0.7741]], requires_grad=True)\n",
      "B\n",
      "tensor([0.1874, 0.9524, 0.5912, 0.1154], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4172, 0.6906, 0.9422, 0.3026],\n",
      "        [0.5388, 0.9842, 0.4073, 0.1801],\n",
      "        [0.2019, 0.3316, 0.7906, 0.3835],\n",
      "        [0.6726, 0.8816, 0.1911, 0.7886]], requires_grad=True)\n",
      "B\n",
      "tensor([0.1841, 0.9517, 0.5963, 0.1203], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4166, 0.6890, 0.9419, 0.2934],\n",
      "        [0.5387, 0.9839, 0.4073, 0.1781],\n",
      "        [0.2029, 0.3341, 0.7910, 0.3980],\n",
      "        [0.6735, 0.8839, 0.1916, 0.8024]], requires_grad=True)\n",
      "B\n",
      "tensor([0.1811, 0.9510, 0.6012, 0.1249], requires_grad=True)\n",
      "Parameters after one update\n",
      "W matrix\n",
      "tensor([[0.4160, 0.6876, 0.9416, 0.2846],\n",
      "        [0.5386, 0.9836, 0.4072, 0.1762],\n",
      "        [0.2038, 0.3363, 0.7915, 0.4118],\n",
      "        [0.6744, 0.8861, 0.1920, 0.8155]], requires_grad=True)\n",
      "B\n",
      "tensor([0.1782, 0.9504, 0.6057, 0.1292], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "while L >= accuracy:\n",
    "    Y_pred = model(X,W,B)\n",
    "    Y_pred = Y_pred.to(torch.float32)\n",
    "    L = loss(Y_pred,Y)\n",
    "    Loss_history.append(L)\n",
    "    \n",
    "    L.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        B -= learning_rate * B.grad\n",
    "    W.grad.zero_() \n",
    "    B.grad.zero_()\n",
    "    print('Parameters after one update')\n",
    "    print('W matrix')\n",
    "    print(W)\n",
    "    print('B')\n",
    "    print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4957, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_history[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "Now we implement the gradient descent on a neural network. We use a simple MLP model to conduct a regression task. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sklearn.datasets.make_regression` to randomly generate a regression problem. `X` is a $100\\times 3$ matrix where each row is a sample. `y` is a $100\\times 1$ lable matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=1, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to cast `X`, `y` to float Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(X)\n",
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.float()\n",
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(y).float()\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'in_features':3,\n",
    "    'lr':0.01,\n",
    "    'n_epoch':100\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config['in_features'],2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "For neural networks, we can use `optimizer` to update our model parameters. It is worth noting that we no longer set the condition `while loss >= accuracy` when optimizing our neural network since we cannot expect how long it takes our model to reduce the loss or even we don't know whether the convergence of neural network is guaranteed, given model hyperparatmeter.\n",
    "\n",
    "With optimizer, we don't we to update parameters manually. All operation is packed in `optimizer.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['lr']) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that we have to enter training mode before our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=2, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 6868.1567\n",
      "epoch: 20, loss = 6868.1465\n",
      "epoch: 30, loss = 6868.1382\n",
      "epoch: 40, loss = 6868.1318\n",
      "epoch: 50, loss = 6868.1274\n",
      "epoch: 60, loss = 6868.1226\n",
      "epoch: 70, loss = 6868.1182\n",
      "epoch: 80, loss = 6868.1138\n",
      "epoch: 90, loss = 6868.1104\n",
      "epoch: 100, loss = 6868.1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lerong/miniconda3/envs/general_env/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config['n_epoch']):\n",
    "    y_pred = model(X)\n",
    "    L = criterion(y_pred,y)\n",
    "\n",
    "    # calculate partial derivatives\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {L.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predicted = model(X).detach().numpy()\n",
    "\n",
    "plt.plot(X, y, 'ro')\n",
    "plt.plot(X, predicted, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the implementation tips and details of gradient descent at which I was confused at the first glance. Hope this helps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
