{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "Imagine that you are rolling two fair dice and the outcome of each roll is a probability distribution:\n",
    "$$\n",
    "f(x)=g(y)=\n",
    "\\begin{cases}\n",
    "\\frac{1}{6}, & \\text{if }x\\in\\{1,2,3,4,5,6\\} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "The distribution $g$ may be different from $f$ if $g$ has certain weights on certain values. Then the probability of rolling 4 in total could be\n",
    "$$\n",
    "f(1)g(3) = \\frac{1}{36} .\n",
    "$$\n",
    "To find the *total likelihood* of two dice summing 4, we have consider all possible values of the two dice, that is, we consider all possible partitions of 4. Thus, the probability of two dice summing 4 is \n",
    "$$\n",
    "f(1)g(3)+f(2)g(2)+f(3)g(1) = \\sum_{x+y=4}f(x)g(y) = \\frac{1}{12}.\n",
    "$$\n",
    "This is exactly a convolution. In general, a convolution evaluated at $c$ is defined as\n",
    "$$\n",
    "(f*g)(c)\\dot{=}\\sum_{x+y=c}f(x)g(y),\n",
    "$$\n",
    "or\n",
    "$$\n",
    "(f*g)(c)=\\sum_x f(x)g(c-x). \n",
    "$$\n",
    "For continuous probability distribution, we replace the sum by a integral over the domain\n",
    "$$\n",
    "(f*g)(c)=\\int_{\\mathbb{R}} f(x)g(c-x) dx.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layers in CNN is an application of convolutions for matrices.\n",
    "\n",
    "**Definition** Let $M\\in\\mathbb{R}^{n_1\\times n_2}$ and $K\\in\\mathbb{R}^{m_1\\times m_2}$ such that $m_1\\leq n_1$, $m_2\\leq n_2$. The **convolution** of $M$ and $K$ is denoted by $Y*K$ with entries\n",
    "$$\n",
    "[M*K]_{i,j}=\\sum_{k=1}^{m_1}\\sum_{l=1}^{m_2} K_{k,l}Y_{i+m_1-k,j+m_2-l}\n",
    "$$\n",
    "for $1\\leq i\\leq n_1-m_1+1$ and $1\\leq j\\leq n_2-m_2+1$. Here $M$ is called the input and $K$ is called the **kernel**. Typically, the kernel is usually square, say $m\\times m$, and it's a hyperparameter of $m^2$ parameters of CNN.\n",
    "\n",
    "**Example** Suppose we have a data matrix\n",
    "$$\n",
    "M=\n",
    "\\left[\\begin{matrix}\n",
    "1 & 5 & -2 & 0 & 2 \\\\\n",
    "3 & 8 & 7 & 1 & 0 \\\\\n",
    "−1 & 0 & 1 & 2 & 3 \\\\\n",
    "4 & 2 & 1 & −1 & 2 \\\\\n",
    "\\end{matrix}\\right].\n",
    "$$\n",
    "and a kernel matrix\n",
    "$$\n",
    "K=\n",
    "\\left[\\begin{matrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9 \\\\\n",
    "\\end{matrix}\\right].\n",
    "$$\n",
    "The definition might look confusing at a glance, but the computation of entries can be roughly said that each element of $M$ is multiplied by the entry of flipped up and down, left and right kernel $K$. \n",
    "$$\n",
    "[M*K]_{1,1}=\n",
    "\\left[\\begin{matrix}\n",
    "1\\cdot 9 & 5\\cdot 8 & -2\\cdot 7 & 0 & 2 \\\\\n",
    "3\\cdot 6 & 8\\cdot 5 & 7\\cdot 4 & 1 & 0 \\\\\n",
    "−1\\cdot 3 & 0\\cdot 2 & 1\\cdot 1 & 2 & 3 \\\\\n",
    "4 & 2 & 1 & −1 & 2 \\\\\n",
    "\\end{matrix}\\right]\n",
    "=9+40-14+18+40+28-3+0+1=119\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Stride\n",
    "The convolution operation will reduce the dimension of the input matrix. Sometimes we hope that the dimension is not reduced quickly by convolutions or even retain the original size. We can use **(zero) padding** $p\\in\\mathbb{N}$ to keep the shape of matrix. For $p=1$, the matrix $M$ becomes\n",
    "$$\n",
    "M=\n",
    "\\left[\\begin{matrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 5 & -2 & 0 & 2 & 0 \\\\\n",
    "0 & 3 & 8 & 7 & 1 & 0 & 0 \\\\\n",
    "0 & −1 & 0 & 1 & 2 & 3 & 0 \\\\\n",
    "0 & 4 & 2 & 1 & −1 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \n",
    "\\end{matrix}\\right].\n",
    "$$\n",
    "We can also set an integer **stride**, which means how far to move the kernel. Usually, the kernel moves from left to right, top to down, and performs a convolution with the corresponding submatrix. For example, when the stride is 2, the kernel moves as the following:\n",
    "\n",
    "![stride=2](images/stride.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stride=2](CNN/images/stride.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(red: before, green: after)\n",
    "\n",
    "In general, a stride $s>1$ reduces the dimension of the output of a convolution to\n",
    "$$\n",
    "\\left(\\frac{n_1-m+1}{s}\\right)\\times\\left(\\frac{n_2-m+1}{s}\\right).\n",
    "$$\n",
    "If we have $N$ filters, then there are $N\\cdot m^2$ variables and the output size of a convolution is\n",
    "$$\n",
    "\\left(\\frac{n_1+2p-m}{s}\\times\\frac{n_2+2p-m}{s}\\right)\\times N\n",
    "$$\n",
    "where the results for all M filters are stacked."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-dimensional Convolutional Layers\n",
    "Now we introduce the 3-dimensional convolutional layers. Let the input tensor (yes, it's no longer a matrix; it's a tensor) be of size $n_1\\times n_2\\times n_3$, and the kernel of size $m\\times m\\times n_3$, i.e. the depth is the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
